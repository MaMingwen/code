{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires\n",
    "\n",
    "## Loading Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attribute of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decode newswire back to text\n",
    " ```\n",
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "# show\n",
    "decoded_review\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "train_data[0]])\n",
    "print(decoded_newswire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "vectorize data\n",
    "\n",
    "Note:\n",
    "```\n",
    "numpy.zeros(shape, dtype=float, order='C')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot method to vectorize labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_labels(labels, dimension = 46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_y_train = vectorize_labels(train_labels)\n",
    "one_hot_y_test = vectorize_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bulit-in way to do above in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "ont_hot_y_train = to_categorical(train_labels)\n",
    "one_hot_y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model conpile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validating your approach\n",
    "\n",
    "Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_y_train[:1000]\n",
    "partial_y_train = one_hot_y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 20, \n",
    "                    batch_size = 512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting the traing and validation acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The network begins to overfit after nine epochs. Letâ€™s train a new network from\n",
    "scratch for nine epochs and then evaluate it on the test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19723953695458593"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "float(np.sum(hits_array)) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genrate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(predictions, predictions[0].shape, np.sum(predictions[0]), np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle labels and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model with an information bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 1s 7ms/step - loss: 3.3727 - accuracy: 0.1267 - val_loss: 2.8595 - val_accuracy: 0.4100\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.3001 - accuracy: 0.4540 - val_loss: 1.8846 - val_accuracy: 0.4670\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.5596 - accuracy: 0.6061 - val_loss: 1.4739 - val_accuracy: 0.6660\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.2335 - accuracy: 0.7083 - val_loss: 1.3376 - val_accuracy: 0.6950\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0684 - accuracy: 0.7425 - val_loss: 1.2723 - val_accuracy: 0.7060\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.9628 - accuracy: 0.7627 - val_loss: 1.2551 - val_accuracy: 0.7210\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8826 - accuracy: 0.7858 - val_loss: 1.2420 - val_accuracy: 0.7240\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.8137 - accuracy: 0.8053 - val_loss: 1.2643 - val_accuracy: 0.7270\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7550 - accuracy: 0.8207 - val_loss: 1.2878 - val_accuracy: 0.7290\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7007 - accuracy: 0.8324 - val_loss: 1.3230 - val_accuracy: 0.7320\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6552 - accuracy: 0.8409 - val_loss: 1.3739 - val_accuracy: 0.7270\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6146 - accuracy: 0.8485 - val_loss: 1.3684 - val_accuracy: 0.7410\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5790 - accuracy: 0.8520 - val_loss: 1.4355 - val_accuracy: 0.7350\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5504 - accuracy: 0.8628 - val_loss: 1.5278 - val_accuracy: 0.7300\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5261 - accuracy: 0.8698 - val_loss: 1.5800 - val_accuracy: 0.7250\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5034 - accuracy: 0.8720 - val_loss: 1.6048 - val_accuracy: 0.7310\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4841 - accuracy: 0.8771 - val_loss: 1.6477 - val_accuracy: 0.7260\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4647 - accuracy: 0.8781 - val_loss: 1.7167 - val_accuracy: 0.7210\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.8834 - val_loss: 1.7700 - val_accuracy: 0.7180\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8840 - val_loss: 1.8071 - val_accuracy: 0.7220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20bb44d5030>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "partial_y_train,\n",
    "epochs=20,\n",
    "batch_size=128,\n",
    "validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Tensorflow2ForKeras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0d7a49511f383ed8cc941752f3a4ccd7b092af867f619c2bdeef89cb64c467a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
